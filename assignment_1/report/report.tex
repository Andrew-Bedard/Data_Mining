\documentclass[english, a4paper]{article}

\usepackage{float}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}


\title{Data Mining -- Assignment 1}
\author{Andrew Bedard -- Artagan Malsagov -- Shabaz Sultan}
\begin{document}
\maketitle
\section{Titanic Survivors\\ \large or: How I learned to Stop Worrying and Love the Data}
\subsection{Introduction}
On the 14th of April, 1912 the RMS Titanic hit an iceberg and sank a few hours later. Of the 2207 people on that ship 1501 lost their lives on that night. These tragic events offer the opportunity to study exactly how people behave during such a life-and-death event.\\
From an economic perspective one can wonder if the model of humans as `homo economicus', that of humans as rational, self-interested actors is the best way to model behaviour of people on that night. An econometric analysis shows that such a `homo economicus' model is overly simplistic because females and children were more likely to survive than physically stronger males. An analysis using said model is still valuable however because people who were closer to their prime age, were of higher social class or had access to more information were more likely to survive\cite{Frey2010}\cite{Frey2011}. \\
The reason why the deathrate was so high starts with the fact that there were not enough lifeboats. On top of that fewer passengers survived than there was space in lifeboats because of supposed reluctance to leave the ship (e.g. because of disbelief that the ship would actually sink or wives that did not want to be separated from their husbands). The difference in survival between males and females can be explained as a result of policy. The official explanation from the Mersey inquiry to explain the difference in survival rates between classes was that lower-class people were less willing to be parted from their belongings and that their English was poorer making them less able to follow orders from the crew. Statistical analysis based on nationality as a proxy for language ability refutes this second claim and suggest that explanations rejected by the inquiry (layout of the ship disadvantaging lower class people and outright discrimination when letting people on lifeboards) are more likely \cite{Hall1986}.\\
We can use publicly available data with personal details of Titanic passengers to build our own models to see if we are able to predict survival of a passenger based on things like sex and class.
\subsection{Analysis of the Data}
The data used in this section is provided through the Kaggle website, which host machine learning competitions. The Titanic dataset is intended as one that users can use to practice and get up to speed with machine learning. The data is provided as a list of 891 passengers in a training set and 418 passengers in a test set. Both sets have a number of attributes marked, mentioned in table~\ref{tab:passenger_attributes}.
\begin{table}[H]
\caption{Attributes proved for each passenger in the dataset.}
\label{tab:passenger_attributes}
\centering
\begin{tabular}{ r | l }
  attribute & possible values \\ \hline \hline
  passenger class & 1,2 or 3 (1 is upper class, 3 is lower)  \\
  name & first and last name with title \\
  & (i.e. mr., miss. etc) and possible initials  \\
  sex & male or female \\
  age & number in years \\
  siblings \& spouses & number of siblings and spouses on board \\
  parents \& children & number of parents and children on board \\
  ticket & ticket number \\
  fare & ticket fare \\
  cabin & cabin code (not available for some, multiple for some)\\
  port of embarkation & Cherbourg, Queenstown or Southampton

\end{tabular}
\end{table}
In addition to the attributes mentioned in table~\ref{tab:passenger_attributes} the passengers in the training set also have an attribute denoting if they survived. This allows us to use the training set as the input for a supervised learning model and use said model to create predictions for survival on the test set. The Kaggle system allows said predictions to submitted and percentage of correct predictions on the test set get displayed in a leaderboard. It should be noted that because this is a dataset available fully marked up (i.e. with survival marked for all passengers) it is trivial to cheat and get a 100\% score. As such the leaderboard is less reliable than other Kaggle competitions and is only intended for practice by Kaggle. 
\subsubsection{Statistical Exploration of Data}
We can start by gathering some basic statistics on the training set to get a sense of the dataset and the distributions for certain attributes. In the trainingset $38.4\%$ survived. There are 216 first class passengers, 184 second class passengers and 491 third class passengers in the set. The passengers are $64.8\%$ male and $35.2\%$ female.\\
Based on both intuition and the literature sex and class seem like the most likely candidates for predicting the survival of a passenger. Because these are also the most straightforward attributes to analyse it makes sense to start with them when first exploring correlations in the dataset. We can start by looking at the correlation coefficient between sex and survival, which is $0.54$, meaning there is a positive correlation between being female and surviving (male and non-survival are encoded as $0$, female and survival as $1$).\\
Next we look at class and survival. There is a positive correlation between being a first class passenger and surviving, with a correlation coefficient of $0.29$. There is a much lower positive correlation for second class passengers of $0.09$ and a negative correlation of $-0.32$ for being a third class passenger and surviving.
\begin{figure}[H]
    \includegraphics[width=0.8\linewidth]{age_distribution}
    \caption{Distribution of ages in training set.}
    \label{fig:age_histogram}.
\end{figure}
\noindent
Only 714 of the 891 passengers in the training set have their age marked. The average age of these 714 passengers is 29.7 years, with a standard deviation of 14.5. The distribution of ages is plotted in a histogram in figure~\ref{fig:age_histogram}.\\
Next we can look if there is difference between the age distribution between survivors and non-survivors. Judging by the naked eye the two histograms look pretty similar and indeed the mean and standard deviation are pretty close as well. There is perhaps a slight difference in the shape of the two distributions, which is expressed as a difference in the kurtosis.\\
Based on this analysis it seems like age would not have great predictive value for survival. It might however still be true that age is more predictive after the population has been split up based on other criteria first.
\begin{figure}[H]
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=\linewidth]{non_survivor_age_distribution}
        \subcaption{Age distribution of non-survivors. Mean $30.6$, std.dev. $14.2$, kurtosis $0.28$.}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=\linewidth]{survivor_age_distribution}
        \subcaption{Age distribution of survivors. Mean $28.3$, std.dev. $15.0$, kurtosis $-0.06$.}
    \end{subfigure}
    \caption{Age distributions of survivors and non-survivors in the training set.}
    \label{fig:survivor_split_age_histograms}
\end{figure}
Finally we look at embarkation ports. Based on intuition it seems less likely that this attribute is particularly predictive of survival. Looking at the data however, passengers who embarked from Southampton had a negative correlation with survival at $-0.16$. Passengers from Cherbourg had a positive survival correlation at $0.17$ and those from Queenstown had almost no correlation at $0.00$. One can wonder if these differences can be explained by a difference in e.g. sex and class between passengers from different ports. One way to explore this is building a decision tree with embarkation port, sex and class as attributes and see how far in the tree embarkation port ends up at, which we will try in section~\ref{subsec:titanic_machine_learning_models}.
\subsubsection{Data Augmentation}
REPLACE in particular building up family trees
\subsection{Machine Learning Models}
\label{subsec:titanic_machine_learning_models}
probably decision trees and logistic regression at the very least
\bibliographystyle{plain}
\bibliography{report}
\end{document}
